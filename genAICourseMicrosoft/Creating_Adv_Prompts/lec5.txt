‚û°Ô∏è Techniques for prompting

There are some basic techniques that we can use to prompt an LLM:

‚óΩZero-shot prompting
‚Ä¢This is the most basic form of prompting 
‚Ä¢It's a single prompt requesting a response from the LLM based solely on its training data.

‚óΩFew-shot prompting
‚Ä¢This type of prompting guides the LLM by providing 1 or more examples it can rely on to generate its response.

‚óΩChain-of-thought 
‚Ä¢This type of prompting tells the LLM how to break down a problem into steps.
‚Ä¢We guide AI model to take steps like first do A, then B, then C and so on to reach the solution 
‚Ä¢The model will mimic that and can take on more advance problems. 


‚óΩGenerated knowledge
‚Ä¢to improve the response of a prompt
‚Ä¢Can provide generated facts or knowledge additionally to your prompt.

‚óΩLeast to most (like chain-of-thought)
‚Ä¢This technique is about breaking down a problem into a series of steps and then ask these steps to be performed in order.
‚Ä¢It is used in data science where it is known that which steps are going to happen in what order 

üî∂Below two for validating response : 

‚óΩSelf-refine, 
‚Ä¢This technique is about critiquing the LLM's output and then asking it to improve.
‚Ä¢We're asking that are you sure about these facts? 
‚Ä¢How about going for a another round!! Give more iterations 

‚óΩMaieutic prompting
‚Ä¢What you want here is to ensure the LLM answer is correct and you ask it to explain various parts of the answer. This is a form of self-refine.
‚Ä¢Break down answer to different parts and for every part we are trying to ascertain that the response is correct or not . 



‚û°Ô∏èLet's deep dive into the chain of thoughts:
So for example if we give our LLM a prompt something like do some math problem (long statement) then "some" LLM basically gives us a wrong answer.

- Prompt: "Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?"
- Answer: 5 ‚úñÔ∏è

‚Ä¢So to improve this we need to tell the exact process of how the question is being done 
‚Ä¢We will provide a similar question to the LLM and show it the whole calculation that how it is being solved 
‚Ä¢Once we do this then if we give our original prompt then the llm will give us the correct answer.

This is what chain of thoughts is !!!



‚û°Ô∏è Let us deep dive into the generated knowledge prompting:
‚Ä¢ This allows us to provide information to our question as you need it.
‚Ä¢ So in this what we do is we provide LLM with an external data 

‚Ä¢The idea is we first fetch the company data that we have and then we insert the data to the llm in the form of template and then we ask a particular question so it will hopefully give a better response.


